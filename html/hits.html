<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>hits API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>hits</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import networkx as nx
import re

import numpy as np
from tabulate import tabulate
from nltk.corpus import stopwords

web_graph = nx.read_gpickle(&#39;web_graph.gpickle&#39;)


def generate_sets(query_word, postings_list):
    &#34;&#34;&#34;
    Generates the root and base sets for the query word

    Takes the entire posting list of the query word as root set.
    Initialises the base set with root set.
    Then iterates through all the edges in the given graph. If at least one of them is in the root set,
    it adds the other to the base set.
    &#34;&#34;&#34;
    if query_word not in postings_list:
        raise Exception(&#39;Word not in the postings list&#39;)
    root_set = postings_list[query_word]
    base_set = []
    for node in root_set:
        base_set.append(node)
    root_set = set(root_set)
    base_set = set(base_set)
    for edge in web_graph.edges:
        if edge[0] in root_set:
            base_set.add(edge[1])
        if edge[1] in root_set:
            base_set.add(edge[0])
    return root_set, base_set


def generate_adj_matrix(base_set):
    &#34;&#34;&#34;
    Generates the adjacency matrix.
    Takes the base set, then iterates through all the edges in the edge set.
    If both endpoints of some edge (a,b) are present in base set, it sets
    adj[a][b] to 1, indicating that there&#39;s an edge from a to b.
    &#34;&#34;&#34;
    adj = np.zeros((len(base_set), len(base_set)))
    bslist = list(base_set)
    for edge in web_graph.edges:
        if edge[0] in base_set and edge[1] in base_set:
            adj[bslist.index(edge[0])][bslist.index(edge[1])] = 1
    return adj


def generate_hub_authority_scores(adj_matrix):
    &#34;&#34;&#34;
    Generates the hub and authority scores.
    It takes the adjacency matrix and then finds the left eigenvector directly using numpy, i.e. the
    eigenvector corresponding to the largest eigenvalue, of (AT)A to find the authority scores,
    and of A(AT) to find the hub scores. Here A denotes the adjacency matrix, and AT denotes its transpose.

    &#34;&#34;&#34;
    aTa = np.dot(adj_matrix.T, adj_matrix)
    aaT = np.dot(adj_matrix, adj_matrix.T)
    v, V = np.linalg.eig(aaT.T)
    left_vec = V[:, 0].T
    left_vec = V[:, v.argmax()]
    left_vec = left_vec / sum(left_vec)
    h_vec = np.reshape(left_vec, (1, -1))
    v, V = np.linalg.eig(aTa.T)
    left_vec = V[:, 0].T
    left_vec = V[:, v.argmax()]
    left_vec = left_vec / sum(left_vec)
    a_vec = np.reshape(left_vec, (1, -1))
    # Return the principal left eigenvector
    return a_vec, h_vec


def generate_hub_authority_scores_power_iteration(adj_matrix):
    &#34;&#34;&#34;
    Generates the hub and authority scores.
    It takes the adjacency matrix and then finds the left eigenvector using the power iteration method, i.e. the
    eigenvector corresponding to the largest eigenvalue of (AT)A to find the authority scores,
    and of A(AT) to find the hub scores. Here A denotes the adjacency matrix, and AT denotes its transpose.

    For this, we initialise our authority and hub scores with 1/n, where n is the number of nodes.
    Then, we multiply authority and hub scores by aTa and aaT respectively, and normalise the result
    by dividing it by the largest value in the matrix.
    We do these iterations until the change after iteration becomes smaller than some threshold value.
    However, in this implementation we have run the iteration 1000 times, as this generally guarantees convergence.
    Then we divide the resulting scores with the sum of all scores, to bring them in the range [0,1], and then
    return them.
    &#34;&#34;&#34;
    aTa = np.dot(adj_matrix.T, adj_matrix)
    aaT = np.dot(adj_matrix, adj_matrix.T)
    a_vec = np.full((1, len(adj_matrix)), 1 / len(adj_matrix))
    h_vec = np.full((1, len(adj_matrix)), 1 / len(adj_matrix))
    for i in range(1000):
        a_vec = np.dot(a_vec, aTa)
        a_vec = a_vec / np.max(a_vec)
        h_vec = np.dot(h_vec, aaT)
        h_vec = h_vec / np.max(h_vec)
    a_vec = a_vec / sum(a_vec[0])
    h_vec = h_vec / sum(h_vec[0])
    return a_vec, h_vec


&#34;&#34;&#34;
Reading the dataset from web_graph.gpickle
&#34;&#34;&#34;
web_graph = nx.read_gpickle(&#39;web_graph.gpickle&#39;)
# Create a postings list
postings_list = {}
stop_words = set(stopwords.words(&#39;english&#39;))

&#34;&#34;&#34;
Creating a posting list by iterating through all nodes, then doing some pre processing like
removing non-alphabetical characters, removing multiple spaces, and converting to lowercase.
Then we filter out all the stop words, and creating posting list for the other words.
&#34;&#34;&#34;
for i in range(len(web_graph.nodes)):
    node = web_graph.nodes[i]
    content = node[&#39;page_content&#39;]
    content = re.sub(r&#39;[^\w\s]&#39;, &#39;&#39;, content)
    content = re.sub(r&#39;\s+&#39;, &#39; &#39;, content)
    content = content.lower()
    content = [c for c in content.split(&#39; &#39;) if c != &#39;&#39; and len(c) &gt; 2]
    for word in content:
        if word not in stop_words:
            if word not in postings_list:
                postings_list[word] = [i]
            elif postings_list[word][-1] != i:
                postings_list[word].append(i)

&#34;&#34;&#34;
Taking input from user.
Then we generate the root and base sets for the given query term.
Then we generate the adjacency matrix using the obtained base set, and
use the adjacency matrix to find authority and hub values.
Finally we print the obtained authority and hub values in a tabulated format.
&#34;&#34;&#34;
query = input(&#34;Enter query word: &#34;)
query = query.split()
query = query[0]
rs, bs = generate_sets(query, postings_list)
adjacency_list = generate_adj_matrix(bs)

authority, hub = generate_hub_authority_scores_power_iteration(adjacency_list)
base_set_list = list(bs)
scores = []
scores_auth = []
scores_hub = []
auth_sum = 0
hub_sum = 0
for i in range(len(authority[0])):
    if authority[0][i] &lt;= 0:
        authority[0][i] = authority[0][i] * -1
    if hub[0][i] &lt; 0:
        hub[0][i] = hub[0][i] * -1
    scores.append([base_set_list[i], authority[0][i], hub[0][i]])
    scores_auth.append([base_set_list[i], authority[0][i]])
    scores_hub.append([base_set_list[i], hub[0][i]])
    auth_sum = auth_sum + authority[0][i]
    hub_sum = hub_sum + hub[0][i]

scores_auth = sorted(scores_auth, key=lambda x: x[1], reverse=True)
scores_auth = scores_auth[0:3]
scores_hub = sorted(scores_hub, key=lambda x: x[1], reverse=True)
scores_hub = scores_hub[0:3]

print(&#34;Top 3 authority scores are:&#34;)
print(tabulate(scores_auth, headers=[&#34;Node&#34;, &#34;Authority Score&#34;]))
print(&#34;\nTop 3 Hub scores are:&#34;)
print(tabulate(scores_hub, headers=[&#34;Node&#34;, &#34;Hub Score&#34;]))

print(&#34;\n List of all scores:&#34;)
print(tabulate(scores, headers=[&#34;Node&#34;, &#34;Authority Score&#34;, &#34;Hub Score&#34;]))
print(&#34;Authority score sum = &#34; + str(auth_sum))
print(&#34;Hub score sum = &#34; + str(hub_sum))</code></pre>
</details>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-variables">Global variables</h2>
<dl>
<dt id="hits.stop_words"><code class="name">var <span class="ident">stop_words</span></code></dt>
<dd>
<div class="desc"><p>Creating a posting list by iterating through all nodes, then doing some pre processing like
removing non-alphabetical characters, removing multiple spaces, and converting to lowercase.
Then we filter out all the stop words, and creating posting list for the other words.</p></div>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="hits.generate_adj_matrix"><code class="name flex">
<span>def <span class="ident">generate_adj_matrix</span></span>(<span>base_set)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates the adjacency matrix.
Takes the base set, then iterates through all the edges in the edge set.
If both endpoints of some edge (a,b) are present in base set, it sets
adj[a][b] to 1, indicating that there's an edge from a to b.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_adj_matrix(base_set):
    &#34;&#34;&#34;
    Generates the adjacency matrix.
    Takes the base set, then iterates through all the edges in the edge set.
    If both endpoints of some edge (a,b) are present in base set, it sets
    adj[a][b] to 1, indicating that there&#39;s an edge from a to b.
    &#34;&#34;&#34;
    adj = np.zeros((len(base_set), len(base_set)))
    bslist = list(base_set)
    for edge in web_graph.edges:
        if edge[0] in base_set and edge[1] in base_set:
            adj[bslist.index(edge[0])][bslist.index(edge[1])] = 1
    return adj</code></pre>
</details>
</dd>
<dt id="hits.generate_hub_authority_scores"><code class="name flex">
<span>def <span class="ident">generate_hub_authority_scores</span></span>(<span>adj_matrix)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates the hub and authority scores.
It takes the adjacency matrix and then finds the left eigenvector directly using numpy, i.e. the
eigenvector corresponding to the largest eigenvalue, of (AT)A to find the authority scores,
and of A(AT) to find the hub scores. Here A denotes the adjacency matrix, and AT denotes its transpose.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_hub_authority_scores(adj_matrix):
    &#34;&#34;&#34;
    Generates the hub and authority scores.
    It takes the adjacency matrix and then finds the left eigenvector directly using numpy, i.e. the
    eigenvector corresponding to the largest eigenvalue, of (AT)A to find the authority scores,
    and of A(AT) to find the hub scores. Here A denotes the adjacency matrix, and AT denotes its transpose.

    &#34;&#34;&#34;
    aTa = np.dot(adj_matrix.T, adj_matrix)
    aaT = np.dot(adj_matrix, adj_matrix.T)
    v, V = np.linalg.eig(aaT.T)
    left_vec = V[:, 0].T
    left_vec = V[:, v.argmax()]
    left_vec = left_vec / sum(left_vec)
    h_vec = np.reshape(left_vec, (1, -1))
    v, V = np.linalg.eig(aTa.T)
    left_vec = V[:, 0].T
    left_vec = V[:, v.argmax()]
    left_vec = left_vec / sum(left_vec)
    a_vec = np.reshape(left_vec, (1, -1))
    # Return the principal left eigenvector
    return a_vec, h_vec</code></pre>
</details>
</dd>
<dt id="hits.generate_hub_authority_scores_power_iteration"><code class="name flex">
<span>def <span class="ident">generate_hub_authority_scores_power_iteration</span></span>(<span>adj_matrix)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates the hub and authority scores.
It takes the adjacency matrix and then finds the left eigenvector using the power iteration method, i.e. the
eigenvector corresponding to the largest eigenvalue of (AT)A to find the authority scores,
and of A(AT) to find the hub scores. Here A denotes the adjacency matrix, and AT denotes its transpose.</p>
<p>For this, we initialise our authority and hub scores with 1/n, where n is the number of nodes.
Then, we multiply authority and hub scores by aTa and aaT respectively, and normalise the result
by dividing it by the largest value in the matrix.
We do these iterations until the change after iteration becomes smaller than some threshold value.
However, in this implementation we have run the iteration 1000 times, as this generally guarantees convergence.
Then we divide the resulting scores with the sum of all scores, to bring them in the range [0,1], and then
return them.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_hub_authority_scores_power_iteration(adj_matrix):
    &#34;&#34;&#34;
    Generates the hub and authority scores.
    It takes the adjacency matrix and then finds the left eigenvector using the power iteration method, i.e. the
    eigenvector corresponding to the largest eigenvalue of (AT)A to find the authority scores,
    and of A(AT) to find the hub scores. Here A denotes the adjacency matrix, and AT denotes its transpose.

    For this, we initialise our authority and hub scores with 1/n, where n is the number of nodes.
    Then, we multiply authority and hub scores by aTa and aaT respectively, and normalise the result
    by dividing it by the largest value in the matrix.
    We do these iterations until the change after iteration becomes smaller than some threshold value.
    However, in this implementation we have run the iteration 1000 times, as this generally guarantees convergence.
    Then we divide the resulting scores with the sum of all scores, to bring them in the range [0,1], and then
    return them.
    &#34;&#34;&#34;
    aTa = np.dot(adj_matrix.T, adj_matrix)
    aaT = np.dot(adj_matrix, adj_matrix.T)
    a_vec = np.full((1, len(adj_matrix)), 1 / len(adj_matrix))
    h_vec = np.full((1, len(adj_matrix)), 1 / len(adj_matrix))
    for i in range(1000):
        a_vec = np.dot(a_vec, aTa)
        a_vec = a_vec / np.max(a_vec)
        h_vec = np.dot(h_vec, aaT)
        h_vec = h_vec / np.max(h_vec)
    a_vec = a_vec / sum(a_vec[0])
    h_vec = h_vec / sum(h_vec[0])
    return a_vec, h_vec</code></pre>
</details>
</dd>
<dt id="hits.generate_sets"><code class="name flex">
<span>def <span class="ident">generate_sets</span></span>(<span>query_word, postings_list)</span>
</code></dt>
<dd>
<div class="desc"><p>Generates the root and base sets for the query word</p>
<p>Takes the entire posting list of the query word as root set.
Initialises the base set with root set.
Then iterates through all the edges in the given graph. If at least one of them is in the root set,
it adds the other to the base set.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_sets(query_word, postings_list):
    &#34;&#34;&#34;
    Generates the root and base sets for the query word

    Takes the entire posting list of the query word as root set.
    Initialises the base set with root set.
    Then iterates through all the edges in the given graph. If at least one of them is in the root set,
    it adds the other to the base set.
    &#34;&#34;&#34;
    if query_word not in postings_list:
        raise Exception(&#39;Word not in the postings list&#39;)
    root_set = postings_list[query_word]
    base_set = []
    for node in root_set:
        base_set.append(node)
    root_set = set(root_set)
    base_set = set(base_set)
    for edge in web_graph.edges:
        if edge[0] in root_set:
            base_set.add(edge[1])
        if edge[1] in root_set:
            base_set.add(edge[0])
    return root_set, base_set</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-variables">Global variables</a></h3>
<ul class="">
<li><code><a title="hits.stop_words" href="#hits.stop_words">stop_words</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="hits.generate_adj_matrix" href="#hits.generate_adj_matrix">generate_adj_matrix</a></code></li>
<li><code><a title="hits.generate_hub_authority_scores" href="#hits.generate_hub_authority_scores">generate_hub_authority_scores</a></code></li>
<li><code><a title="hits.generate_hub_authority_scores_power_iteration" href="#hits.generate_hub_authority_scores_power_iteration">generate_hub_authority_scores_power_iteration</a></code></li>
<li><code><a title="hits.generate_sets" href="#hits.generate_sets">generate_sets</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>